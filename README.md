NLP practice

1. Folder of news
   kaggle link: https://www.kaggle.com/datasets/kevinmorgado/spanish-news-classification. <br>
   In this Jupyter Notebook I practice the Tokenization, Stemming and Lemmatization tecniques, using the naive bayes model (MultinomialNB). Creating trainings with stemming and lemmatization tecniques so I can      compare both of them with the accuracy results, knowing the lemmatization tecnique have a higher computational cost, but in this case the results were similar, so depends of the porpuse we can select a different tecnique of tokenization.
2. Folder of movies recomendations (recomendacion_peliculas)
    kaggle link: https://www.kaggle.com/datasets/carolzhangdc/imdb-5000-movie-dataset. <br>
In this Jupyter Notebook I practice the TF-IDF method to visualice the Cosine way to calculate the similarity between vectors (cosine_similarity), using matplotlib library to see the type of graph, in this case a logaritming graph, I can do this by taking a movie from this dataset and compare it with the rest of the movies and selecting the first ten movies for movies recommendations.


|  Imagen  | Descripci√≥n |
|---|---|
|  `![image](https://github.com/user-attachments/assets/502a2548-44f4-4976-9e03-a329f36d2dc8)` |  This graph has the data out of order  |
|  `![image](https://github.com/user-attachments/assets/09b505e3-dc3a-4a7d-868f-8c47f39a483d)` | And this graph has a logaritming form due to ordering data from highest to lowest similraity of the vectors from this movies   |
