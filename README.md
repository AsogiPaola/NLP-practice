NLP practice

1. Folder of news
   kaggle link: https://www.kaggle.com/datasets/kevinmorgado/spanish-news-classification. <br>
   In this Jupyter Notebook I practice the Tokenization, Stemming and Lemmatization tecniques, using the naive bayes model (MultinomialNB). Creating trainings with stemming and lemmatization tecniques so I can      compare both of them with the accuracy results, knowing the lemmatization tecnique have a higher computational cost, but in this case the results were similar, so depends of the porpuse we can select a different tecnique of tokenization.
2. Folder of movies recomendations (recomendacion_peliculas)
    kaggle link: https://www.kaggle.com/datasets/carolzhangdc/imdb-5000-movie-dataset. <br>
In this Jupyter Notebook I practice the TF-IDF method to visualice the Cosine way to calculate the similarity between vectors (cosine_similarity), using matplotlib library to see the type of graph, in this case a logaritming graph, I can do this by taking a movie from this dataset and compare it with the rest of the movies and selecting the first ten movies for movies recommendations.


|  Imagen  | Descripci√≥n |
|---|---|
|  `<img src="https://github.com/user-attachments/assets/b392c25b-1adb-441a-94f0-5046801b0d39" width="300" height="300">` |  This graph has the data out of order |
|  `<img src="https://github.com/user-attachments/assets/ff9cbcd3-b2ab-4719-aaa1-d1ab430d4fe7" width="300" height="300">` | And this graph has a logaritming form due to ordering data from highest to lowest similraity of the vectors from this movies |

